{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LDuULwyNZKp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e8133806-903a-4f26-df81-4c6c537b9a7d"
      },
      "source": [
        "import nltk\n",
        " \n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
        " \n",
        "# [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
        "# Tagged sentences:  3914\n",
        "# Tagged words: 100676"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  3914\n",
            "Tagged words: 100676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qrz7Q17NeHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4403439a-ba78-40b7-edf3-3ecc58af1833"
      },
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
        " \n",
        "# [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
        "# Tagged sentences:  3914\n",
        "# Tagged words: 100676"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  3914\n",
            "Tagged words: 100676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb8e-Tg_Nv-d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9324fe6b-f91f-4802-8013-c9ea0c155e8d"
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "sentences, sentence_tags =[], [] \n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        " \n",
        "# Let's see how a sequence looks\n",
        " \n",
        "print(sentences[5])\n",
        "print(sentence_tags[5])\n",
        " \n",
        "# ['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
        "#  'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
        "#  'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
        "# '.']\n",
        "# ['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
        "#  'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
        "#  '.']\n",
        " "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
            " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
            " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
            " '.']\n",
            "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
            " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
            " '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBHc3lHFN3Aa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "24ef26e2-0c1a-477d-88b0-32cab79309ac"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        " \n",
        "(train_sentences, \n",
        " test_sentences, \n",
        " train_tags, \n",
        " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n",
        "\n",
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        " \n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        " \n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # The special value used to padding\n",
        "\n",
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        " \n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    train_sentences_X.append(s_int)\n",
        " \n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        " \n",
        "    test_sentences_X.append(s_int)\n",
        " \n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag2index[t] for t in s])\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])\n",
        " \n",
        "# [2385, 9167, 860, 4989, 6805, 6349, 9078, 3938, 862, 1092, 4799, 860, 1198, 1131, 879, 5014, 7870, 704, 4415, 8049, 9444, 8175, 8172, 10058, 10034, 9890, 1516, 8311, 7870, 1489, 7967, 6458, 8859, 9720, 6754, 5402, 9254, 2663]\n",
        "# [3829, 3347, 1, 8311, 6240, 982, 7936, 1, 3552, 4558, 1, 9007, 8175, 8172, 637, 4517, 7392, 3124, 860, 5416, 920, 3301, 6240, 1205, 5282, 6683, 9890, 758, 4415, 1, 6240, 3386, 9072, 3219, 6240, 9157, 5611, 6240, 6969, 4517, 2956, 175, 2663]\n",
        "# [11, 35, 39, 3, 7, 9, 20, 42, 42, 3, 35, 39, 35, 35, 22, 7, 10, 16, 32, 35, 31, 17, 3, 11, 42, 7, 9, 3, 10, 16, 6, 25, 12, 11, 42, 17, 6, 44]\n",
        "# [2, 35, 16, 3, 20, 35, 42, 42, 16, 25, 7, 31, 17, 3, 35, 15, 42, 7, 39, 35, 35, 16, 20, 42, 40, 16, 7, 6, 32, 30, 20, 42, 42, 37, 20, 42, 3, 20, 42, 15, 11, 42, 44]\n",
        " \n",
        " "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2678, 973, 6882, 2501, 9737, 7648, 4944, 10054, 3467, 2501, 913, 5276, 9701, 3580, 1535, 9251, 4331, 8580, 2501, 6720, 9527, 3836, 2669, 7554, 6022, 9737, 3403, 866, 8955, 3365, 7112]\n",
            "[43, 3342, 6087, 5360, 9562, 5929, 1, 2207, 8776, 8024, 1691, 866, 2003, 34, 1122, 4840, 6882, 10041, 7528, 1, 866, 2501, 6772, 3169, 866, 1, 7112]\n",
            "[23, 30, 28, 24, 12, 30, 13, 28, 39, 24, 21, 23, 30, 39, 3, 28, 18, 43, 24, 21, 3, 3, 12, 39, 21, 12, 21, 39, 21, 23, 36]\n",
            "[44, 31, 21, 23, 38, 31, 37, 14, 24, 21, 23, 39, 23, 45, 9, 38, 28, 28, 5, 5, 39, 24, 5, 5, 39, 5, 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIOJUx0PN_u8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a536fc4a-29ac-4a9f-b134-06ef17abd042"
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH)  # 271"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQxXDdWIOIdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2dad4e8-1d23-46a6-d2cd-673b2f8b42e6"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        " \n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2678   973  6882  2501  9737  7648  4944 10054  3467  2501   913  5276\n",
            "  9701  3580  1535  9251  4331  8580  2501  6720  9527  3836  2669  7554\n",
            "  6022  9737  3403   866  8955  3365  7112     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0]\n",
            "[   43  3342  6087  5360  9562  5929     1  2207  8776  8024  1691   866\n",
            "  2003    34  1122  4840  6882 10041  7528     1   866  2501  6772  3169\n",
            "   866     1  7112     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0]\n",
            "[23 30 28 24 12 30 13 28 39 24 21 23 30 39  3 28 18 43 24 21  3  3 12 39\n",
            " 21 12 21 39 21 23 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n",
            "[44 31 21 23 38 31 37 14 24 21 23 39 23 45  9 38 28 28  5  5 39 24  5  5\n",
            " 39  5 36  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdHBZkmtOxzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "dc1155f7-0d87-46ba-d7bf-c18e041445c4"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 271, 128)          1294720   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 271, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 271, 47)           24111     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 2,107,311\n",
            "Trainable params: 2,107,311\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU51o_ZjPBWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "05816f9c-2389-4947-acea-d7946419801d"
      },
      "source": [
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)\n",
        "\n",
        "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
        "print(cat_train_tags_y[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjXlhsoHPLWr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33f422ba-5770-4b6d-f902-974ee821323b"
      },
      "source": [
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 2504 samples, validate on 627 samples\n",
            "Epoch 1/40\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 1.3228 - acc: 0.8587 - val_loss: 0.3675 - val_acc: 0.9082\n",
            "Epoch 2/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.3376 - acc: 0.9092 - val_loss: 0.3147 - val_acc: 0.9080\n",
            "Epoch 3/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.3135 - acc: 0.9110 - val_loss: 0.3032 - val_acc: 0.9171\n",
            "Epoch 4/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.3030 - acc: 0.9166 - val_loss: 0.2951 - val_acc: 0.9181\n",
            "Epoch 5/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.2948 - acc: 0.9168 - val_loss: 0.2875 - val_acc: 0.9181\n",
            "Epoch 6/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.2867 - acc: 0.9175 - val_loss: 0.2796 - val_acc: 0.9220\n",
            "Epoch 7/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.2794 - acc: 0.9220 - val_loss: 0.2733 - val_acc: 0.9222\n",
            "Epoch 8/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.2739 - acc: 0.9224 - val_loss: 0.2682 - val_acc: 0.9232\n",
            "Epoch 9/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.2688 - acc: 0.9236 - val_loss: 0.2633 - val_acc: 0.9261\n",
            "Epoch 10/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.2633 - acc: 0.9268 - val_loss: 0.2565 - val_acc: 0.9339\n",
            "Epoch 11/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.2530 - acc: 0.9349 - val_loss: 0.2447 - val_acc: 0.9382\n",
            "Epoch 12/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.2366 - acc: 0.9418 - val_loss: 0.2244 - val_acc: 0.9447\n",
            "Epoch 13/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.2119 - acc: 0.9476 - val_loss: 0.1988 - val_acc: 0.9495\n",
            "Epoch 14/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.1832 - acc: 0.9527 - val_loss: 0.1713 - val_acc: 0.9554\n",
            "Epoch 15/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.1549 - acc: 0.9598 - val_loss: 0.1460 - val_acc: 0.9625\n",
            "Epoch 16/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.1294 - acc: 0.9674 - val_loss: 0.1244 - val_acc: 0.9677\n",
            "Epoch 17/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.1078 - acc: 0.9741 - val_loss: 0.1066 - val_acc: 0.9735\n",
            "Epoch 18/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.0890 - acc: 0.9798 - val_loss: 0.0921 - val_acc: 0.9773\n",
            "Epoch 19/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0733 - acc: 0.9842 - val_loss: 0.0795 - val_acc: 0.9817\n",
            "Epoch 20/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.0597 - acc: 0.9882 - val_loss: 0.0691 - val_acc: 0.9845\n",
            "Epoch 21/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0486 - acc: 0.9908 - val_loss: 0.0608 - val_acc: 0.9864\n",
            "Epoch 22/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.0398 - acc: 0.9924 - val_loss: 0.0549 - val_acc: 0.9875\n",
            "Epoch 23/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.0331 - acc: 0.9937 - val_loss: 0.0504 - val_acc: 0.9882\n",
            "Epoch 24/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0280 - acc: 0.9946 - val_loss: 0.0469 - val_acc: 0.9890\n",
            "Epoch 25/40\n",
            "2504/2504 [==============================] - 146s 59ms/step - loss: 0.0239 - acc: 0.9954 - val_loss: 0.0442 - val_acc: 0.9895\n",
            "Epoch 26/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.0209 - acc: 0.9959 - val_loss: 0.0422 - val_acc: 0.9899\n",
            "Epoch 27/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.0183 - acc: 0.9964 - val_loss: 0.0406 - val_acc: 0.9902\n",
            "Epoch 28/40\n",
            "2504/2504 [==============================] - 148s 59ms/step - loss: 0.0163 - acc: 0.9968 - val_loss: 0.0397 - val_acc: 0.9903\n",
            "Epoch 29/40\n",
            "2504/2504 [==============================] - 148s 59ms/step - loss: 0.0147 - acc: 0.9971 - val_loss: 0.0384 - val_acc: 0.9907\n",
            "Epoch 30/40\n",
            "2504/2504 [==============================] - 149s 59ms/step - loss: 0.0133 - acc: 0.9973 - val_loss: 0.0376 - val_acc: 0.9907\n",
            "Epoch 31/40\n",
            "2504/2504 [==============================] - 148s 59ms/step - loss: 0.0121 - acc: 0.9975 - val_loss: 0.0374 - val_acc: 0.9909\n",
            "Epoch 32/40\n",
            "2504/2504 [==============================] - 148s 59ms/step - loss: 0.0110 - acc: 0.9978 - val_loss: 0.0363 - val_acc: 0.9912\n",
            "Epoch 33/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0101 - acc: 0.9980 - val_loss: 0.0363 - val_acc: 0.9910\n",
            "Epoch 34/40\n",
            "2504/2504 [==============================] - 149s 59ms/step - loss: 0.0093 - acc: 0.9981 - val_loss: 0.0361 - val_acc: 0.9912\n",
            "Epoch 35/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0086 - acc: 0.9983 - val_loss: 0.0356 - val_acc: 0.9913\n",
            "Epoch 36/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0080 - acc: 0.9984 - val_loss: 0.0356 - val_acc: 0.9914\n",
            "Epoch 37/40\n",
            "2504/2504 [==============================] - 148s 59ms/step - loss: 0.0074 - acc: 0.9986 - val_loss: 0.0357 - val_acc: 0.9913\n",
            "Epoch 38/40\n",
            "2504/2504 [==============================] - 148s 59ms/step - loss: 0.0069 - acc: 0.9986 - val_loss: 0.0358 - val_acc: 0.9913\n",
            "Epoch 39/40\n",
            "2504/2504 [==============================] - 149s 59ms/step - loss: 0.0064 - acc: 0.9987 - val_loss: 0.0351 - val_acc: 0.9914\n",
            "Epoch 40/40\n",
            "2504/2504 [==============================] - 147s 59ms/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0357 - val_acc: 0.9913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa258570940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMFFH62wmE1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "82d81080-39ce-407f-b7b8-8ae3a8fa6fd6"
      },
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825\n",
        " "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "783/783 [==============================] - 9s 12ms/step\n",
            "acc: 99.10883033412627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpdX0Oa-mPCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c92081d5-e184-4ec2-85cb-7c45c5a258b5"
      },
      "source": [
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "print(test_samples)\n",
        " \n",
        "# [['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n",
        " "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['running', 'is', 'very', 'important', 'for', 'me', '.'], ['I', 'was', 'running', 'every', 'day', 'for', 'a', 'month', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqbdMEF1mb6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "1ec74d75-ec87-4d9a-9677-876b63fa7fb5"
      },
      "source": [
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)\n",
        " "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3899 9562 5649 2187 4988 9932 7112    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [3263 4099 3899 8102 8693 4988 8776 8512 7112    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jricQ7hbmfHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "617df956-d899-4fa9-a5e7-dc30041f277b"
      },
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[1.2547887e-02 1.0553148e-02 5.2794930e-02 ... 2.1123439e-04\n",
            "   6.0600956e-04 3.6907100e-04]\n",
            "  [4.6316934e-05 8.4883377e-06 8.2997634e-04 ... 5.5333810e-05\n",
            "   1.4556192e-04 1.1455044e-05]\n",
            "  [3.7719765e-06 5.3231173e-07 1.4315035e-04 ... 8.5397006e-04\n",
            "   1.1771016e-04 4.4265922e-05]\n",
            "  ...\n",
            "  [9.9995852e-01 1.5566673e-06 7.1499562e-06 ... 5.6737655e-09\n",
            "   2.9733096e-09 1.5398966e-08]\n",
            "  [9.9993241e-01 1.9678805e-06 1.0421809e-05 ... 1.2806732e-08\n",
            "   5.9385101e-09 2.9315293e-08]\n",
            "  [9.9989653e-01 2.4072890e-06 1.4566865e-05 ... 2.6080684e-08\n",
            "   1.0724764e-08 4.9910877e-08]]\n",
            "\n",
            " [[9.5643327e-06 4.0900562e-05 1.4810343e-04 ... 1.7231328e-02\n",
            "   1.9942852e-02 3.9018512e-05]\n",
            "  [1.7758874e-06 2.8649349e-05 2.4394342e-06 ... 9.2919490e-06\n",
            "   1.7359656e-04 4.6051550e-06]\n",
            "  [4.6386645e-05 7.3728079e-05 1.0408045e-03 ... 6.6188892e-07\n",
            "   3.3952649e-06 2.1035511e-04]\n",
            "  ...\n",
            "  [9.9995828e-01 1.5752101e-06 6.9824009e-06 ... 5.6544800e-09\n",
            "   2.8967975e-09 1.5484463e-08]\n",
            "  [9.9993205e-01 1.9913120e-06 1.0177696e-05 ... 1.2763224e-08\n",
            "   5.7859477e-09 2.9478221e-08]\n",
            "  [9.9989629e-01 2.4359413e-06 1.4225811e-05 ... 2.5992183e-08\n",
            "   1.0449720e-08 5.0188660e-08]]] (2, 271, 47)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsLKIdrvmnYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "62ba8adc-9da2-409c-96fe-221ddeb4350a"
      },
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences\n",
        "\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))\n",
        " \n",
        "# ['JJ', 'NNS', 'NN', 'NNP', 'NNP', 'NNS', '-NONE-', '-PAD-', ...\n",
        "# ['VBP', 'CD', 'JJ', 'CD', 'NNS', 'NNP', 'POS', 'NN', '-NONE-', '-PAD-', ...\n",
        " "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['NNP', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F99_z3UOoZZn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e08f886f-de3b-423b-92fb-e9f76ec34b08"
      },
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        " \n",
        " \n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        " \n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        " \n",
        "model.summary()\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 271, 128)          1294720   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 271, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 271, 47)           24111     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 271, 47)           0         \n",
            "=================================================================\n",
            "Total params: 2,107,311\n",
            "Trainable params: 2,107,311\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37C4OemyoqRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3c90dec-52e3-4d57-93ff-cc4c8c9a95f0"
      },
      "source": [
        "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2504 samples, validate on 627 samples\n",
            "Epoch 1/40\n",
            "2504/2504 [==============================] - 148s 59ms/step - loss: 1.3524 - acc: 0.8586 - ignore_accuracy: 0.0079 - val_loss: 0.4144 - val_acc: 0.9064 - val_ignore_accuracy: 0.0527\n",
            "Epoch 2/40\n",
            "2504/2504 [==============================] - 146s 58ms/step - loss: 0.3503 - acc: 0.9047 - ignore_accuracy: 0.0215 - val_loss: 0.3202 - val_acc: 0.9060 - val_ignore_accuracy: 0.0000e+00\n",
            "Epoch 3/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.3179 - acc: 0.9086 - ignore_accuracy: 0.1076 - val_loss: 0.3070 - val_acc: 0.9163 - val_ignore_accuracy: 0.1290\n",
            "Epoch 4/40\n",
            "2504/2504 [==============================] - 146s 58ms/step - loss: 0.3072 - acc: 0.9160 - ignore_accuracy: 0.1325 - val_loss: 0.2991 - val_acc: 0.9180 - val_ignore_accuracy: 0.1350\n",
            "Epoch 5/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.2985 - acc: 0.9163 - ignore_accuracy: 0.1325 - val_loss: 0.2942 - val_acc: 0.9178 - val_ignore_accuracy: 0.1341\n",
            "Epoch 6/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.2902 - acc: 0.9163 - ignore_accuracy: 0.1318 - val_loss: 0.2858 - val_acc: 0.9179 - val_ignore_accuracy: 0.1348\n",
            "Epoch 7/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.2850 - acc: 0.9163 - ignore_accuracy: 0.1317 - val_loss: 0.2820 - val_acc: 0.9196 - val_ignore_accuracy: 0.1513\n",
            "Epoch 8/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.2798 - acc: 0.9188 - ignore_accuracy: 0.1529 - val_loss: 0.2776 - val_acc: 0.9210 - val_ignore_accuracy: 0.1643\n",
            "Epoch 9/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.2747 - acc: 0.9207 - ignore_accuracy: 0.1706 - val_loss: 0.2692 - val_acc: 0.9256 - val_ignore_accuracy: 0.2112\n",
            "Epoch 10/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.2694 - acc: 0.9265 - ignore_accuracy: 0.2320 - val_loss: 0.2629 - val_acc: 0.9326 - val_ignore_accuracy: 0.2853\n",
            "Epoch 11/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.2623 - acc: 0.9340 - ignore_accuracy: 0.3097 - val_loss: 0.2541 - val_acc: 0.9379 - val_ignore_accuracy: 0.3415\n",
            "Epoch 12/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.2519 - acc: 0.9417 - ignore_accuracy: 0.3911 - val_loss: 0.2420 - val_acc: 0.9428 - val_ignore_accuracy: 0.3940\n",
            "Epoch 13/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.2364 - acc: 0.9463 - ignore_accuracy: 0.4383 - val_loss: 0.2238 - val_acc: 0.9480 - val_ignore_accuracy: 0.4500\n",
            "Epoch 14/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.2152 - acc: 0.9496 - ignore_accuracy: 0.4738 - val_loss: 0.2015 - val_acc: 0.9514 - val_ignore_accuracy: 0.4866\n",
            "Epoch 15/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.1906 - acc: 0.9536 - ignore_accuracy: 0.5154 - val_loss: 0.1783 - val_acc: 0.9545 - val_ignore_accuracy: 0.5197\n",
            "Epoch 16/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.1666 - acc: 0.9574 - ignore_accuracy: 0.5559 - val_loss: 0.1567 - val_acc: 0.9587 - val_ignore_accuracy: 0.5636\n",
            "Epoch 17/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.1442 - acc: 0.9620 - ignore_accuracy: 0.6039 - val_loss: 0.1374 - val_acc: 0.9643 - val_ignore_accuracy: 0.6232\n",
            "Epoch 18/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.1235 - acc: 0.9689 - ignore_accuracy: 0.6761 - val_loss: 0.1198 - val_acc: 0.9693 - val_ignore_accuracy: 0.6755\n",
            "Epoch 19/40\n",
            "2504/2504 [==============================] - 147s 59ms/step - loss: 0.1043 - acc: 0.9750 - ignore_accuracy: 0.7396 - val_loss: 0.1036 - val_acc: 0.9738 - val_ignore_accuracy: 0.7224\n",
            "Epoch 20/40\n",
            "2504/2504 [==============================] - 147s 59ms/step - loss: 0.0867 - acc: 0.9801 - ignore_accuracy: 0.7920 - val_loss: 0.0905 - val_acc: 0.9775 - val_ignore_accuracy: 0.7621\n",
            "Epoch 21/40\n",
            "2504/2504 [==============================] - 146s 58ms/step - loss: 0.0714 - acc: 0.9843 - ignore_accuracy: 0.8359 - val_loss: 0.0782 - val_acc: 0.9821 - val_ignore_accuracy: 0.8109\n",
            "Epoch 22/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.0582 - acc: 0.9882 - ignore_accuracy: 0.8764 - val_loss: 0.0679 - val_acc: 0.9850 - val_ignore_accuracy: 0.8416\n",
            "Epoch 23/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0471 - acc: 0.9907 - ignore_accuracy: 0.9031 - val_loss: 0.0598 - val_acc: 0.9867 - val_ignore_accuracy: 0.8593\n",
            "Epoch 24/40\n",
            "2504/2504 [==============================] - 147s 59ms/step - loss: 0.0386 - acc: 0.9925 - ignore_accuracy: 0.9216 - val_loss: 0.0541 - val_acc: 0.9875 - val_ignore_accuracy: 0.8676\n",
            "Epoch 25/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0321 - acc: 0.9937 - ignore_accuracy: 0.9342 - val_loss: 0.0497 - val_acc: 0.9882 - val_ignore_accuracy: 0.8760\n",
            "Epoch 26/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.0272 - acc: 0.9946 - ignore_accuracy: 0.9442 - val_loss: 0.0464 - val_acc: 0.9889 - val_ignore_accuracy: 0.8833\n",
            "Epoch 27/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.0234 - acc: 0.9954 - ignore_accuracy: 0.9516 - val_loss: 0.0438 - val_acc: 0.9894 - val_ignore_accuracy: 0.8879\n",
            "Epoch 28/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.0204 - acc: 0.9959 - ignore_accuracy: 0.9571 - val_loss: 0.0419 - val_acc: 0.9898 - val_ignore_accuracy: 0.8921\n",
            "Epoch 29/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.0180 - acc: 0.9963 - ignore_accuracy: 0.9614 - val_loss: 0.0401 - val_acc: 0.9903 - val_ignore_accuracy: 0.8978\n",
            "Epoch 30/40\n",
            "2504/2504 [==============================] - 145s 58ms/step - loss: 0.0160 - acc: 0.9967 - ignore_accuracy: 0.9659 - val_loss: 0.0388 - val_acc: 0.9906 - val_ignore_accuracy: 0.9001\n",
            "Epoch 31/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.0143 - acc: 0.9971 - ignore_accuracy: 0.9697 - val_loss: 0.0381 - val_acc: 0.9907 - val_ignore_accuracy: 0.9018\n",
            "Epoch 32/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.0130 - acc: 0.9974 - ignore_accuracy: 0.9729 - val_loss: 0.0373 - val_acc: 0.9910 - val_ignore_accuracy: 0.9046\n",
            "Epoch 33/40\n",
            "2504/2504 [==============================] - 144s 58ms/step - loss: 0.0118 - acc: 0.9977 - ignore_accuracy: 0.9756 - val_loss: 0.0364 - val_acc: 0.9913 - val_ignore_accuracy: 0.9073\n",
            "Epoch 34/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.0108 - acc: 0.9979 - ignore_accuracy: 0.9776 - val_loss: 0.0361 - val_acc: 0.9915 - val_ignore_accuracy: 0.9096\n",
            "Epoch 35/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.0099 - acc: 0.9981 - ignore_accuracy: 0.9798 - val_loss: 0.0359 - val_acc: 0.9915 - val_ignore_accuracy: 0.9096\n",
            "Epoch 36/40\n",
            "2504/2504 [==============================] - 144s 57ms/step - loss: 0.0091 - acc: 0.9982 - ignore_accuracy: 0.9814 - val_loss: 0.0357 - val_acc: 0.9914 - val_ignore_accuracy: 0.9093\n",
            "Epoch 37/40\n",
            "2504/2504 [==============================] - 142s 57ms/step - loss: 0.0084 - acc: 0.9984 - ignore_accuracy: 0.9829 - val_loss: 0.0360 - val_acc: 0.9916 - val_ignore_accuracy: 0.9112\n",
            "Epoch 38/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.0077 - acc: 0.9985 - ignore_accuracy: 0.9846 - val_loss: 0.0354 - val_acc: 0.9916 - val_ignore_accuracy: 0.9112\n",
            "Epoch 39/40\n",
            "2504/2504 [==============================] - 143s 57ms/step - loss: 0.0072 - acc: 0.9986 - ignore_accuracy: 0.9857 - val_loss: 0.0352 - val_acc: 0.9917 - val_ignore_accuracy: 0.9120\n",
            "Epoch 40/40\n",
            "2504/2504 [==============================] - 142s 57ms/step - loss: 0.0067 - acc: 0.9987 - ignore_accuracy: 0.9869 - val_loss: 0.0349 - val_acc: 0.9917 - val_ignore_accuracy: 0.9120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa216f8af60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vsb1VfjA4Ah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "599e25a2-6e9d-467d-898d-83457c3ba4cb"
      },
      "source": [
        "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
        "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # ignore_acc"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "783/783 [==============================] - 9s 11ms/step\n",
            "acc: 99.15689928778286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24xishdgBXw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aef1a48f-9c20-4318-c371-63cab351ebdb"
      },
      "source": [
        "test_samples = [\n",
        "    \"running is very important for me .\".split(),\n",
        "    \"I was running every day for a month .\".split()\n",
        "]\n",
        "\n",
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        " \n",
        "test_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "print(test_samples_X)\n",
        "\n",
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)\n",
        "\n",
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences\n",
        "\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3899 9562 5649 2187 4988 9932 7112    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]\n",
            " [3263 4099 3899 8102 8693 4988 8776 8512 7112    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0]]\n",
            "[[[3.23848624e-04 4.97277314e-03 1.40404981e-03 ... 1.02647746e-07\n",
            "   4.28112980e-06 1.19597607e-04]\n",
            "  [2.74535796e-06 4.60777574e-06 3.89154447e-05 ... 1.93413900e-04\n",
            "   1.85135377e-06 1.41144164e-05]\n",
            "  [2.45068804e-05 2.59806643e-06 1.16055518e-04 ... 3.18124985e-05\n",
            "   3.11696203e-05 7.47221275e-05]\n",
            "  ...\n",
            "  [9.99981880e-01 2.16144103e-09 4.61682504e-07 ... 5.93437625e-13\n",
            "   3.27620316e-14 3.01291323e-07]\n",
            "  [9.99969006e-01 2.31566677e-09 1.03425782e-06 ... 1.60060225e-12\n",
            "   8.93867906e-14 5.86530348e-07]\n",
            "  [9.99942660e-01 2.43429943e-09 2.33300079e-06 ... 3.94047468e-12\n",
            "   2.33526740e-13 1.08681843e-06]]\n",
            "\n",
            " [[2.07925732e-05 7.61897594e-04 1.41377211e-06 ... 3.58139368e-05\n",
            "   1.02202125e-06 1.82379190e-05]\n",
            "  [1.63685036e-06 4.02977857e-05 3.72129252e-05 ... 6.01654756e-04\n",
            "   1.20569357e-05 2.49793948e-06]\n",
            "  [2.60247587e-04 8.45220129e-05 4.25182370e-04 ... 2.25071322e-08\n",
            "   1.31368040e-07 3.50772461e-05]\n",
            "  ...\n",
            "  [9.99981880e-01 2.16144103e-09 4.61682077e-07 ... 5.93437625e-13\n",
            "   3.27620316e-14 3.01291038e-07]\n",
            "  [9.99969006e-01 2.31565789e-09 1.03424998e-06 ... 1.60060225e-12\n",
            "   8.93864518e-14 5.86526994e-07]\n",
            "  [9.99942660e-01 2.43429010e-09 2.33298306e-06 ... 3.94045994e-12\n",
            "   2.33525845e-13 1.08681115e-06]]] (2, 271, 47)\n",
            "[['VBG', 'VBZ', 'RB', 'JJ', 'IN', 'PRP', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-'], ['PRP', 'VBD', 'VBG', 'DT', 'NN', 'IN', 'DT', 'NN', '.', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz1egmWxBgAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}